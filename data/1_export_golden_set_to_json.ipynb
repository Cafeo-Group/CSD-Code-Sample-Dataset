{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1f21885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added parent directory 'finer' to sys.path\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import random\n",
    "from typing import List, Dict, Any\n",
    "import logging\n",
    "\n",
    "notebook_dir = os.getcwd()\n",
    "parent_dir = str(Path(notebook_dir).parent)\n",
    "\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "    print(f\"Added parent directory '{Path(parent_dir).name}' to sys.path\")\n",
    "    \n",
    "os.chdir(parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "428f4fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.repository import Repository\n",
    "from models.commit import Commit\n",
    "from models.file import File\n",
    "from models.cf import CommitFile, MetadataHelper\n",
    "from models.hunk import Hunk\n",
    "from utils.worker import get_optimal_max_workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aae82131",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_commmit_link(repo, sha):\n",
    "    return f\"https://github.com/{repo.org_name}/{repo.repo_name}/commit/{sha}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "154e4833",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_metadata(mt, com, name, file_content):\n",
    "    cf = CommitFile(com.repo_name, com.org_name, name, com.sha, file_content, mt.change_type, mt.file_mode, mt.index_info)\n",
    "    hunk = Hunk(None, name, com.repo_name, com.org_name, com.sha, mt.old_start, mt.old_length, mt.new_start, mt.new_length, mt.lines, mt.old_name, mt.new_name)\n",
    "    return cf, hunk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b72fdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_com_cfs_and_hunks(repo_path, com, file_names, workers):\n",
    "    cfs = []\n",
    "    hunks = []\n",
    "\n",
    "    for name in file_names:\n",
    "        file_content, _ = File.get_file_content(repo_path, com.sha, name)\n",
    "        metadata_list = CommitFile.get_metadata(com.org_name, com.repo_name, com.sha, name, True)\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=workers) as executor:\n",
    "            futures = [executor.submit(process_metadata, mt, com, name, file_content) for mt in metadata_list]\n",
    "            for future in futures:\n",
    "                try:\n",
    "                    cf, hunk = future.result()\n",
    "                    cfs.append(cf)\n",
    "                    hunks.append(hunk)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing metadata for file {name}: {e}\")\n",
    "\n",
    "    return cfs, hunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddc45793",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_to_json(coms_data: Dict[str, Dict[str, Dict[str, List[str]]]], num_commits: int, workers: int) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Export repository data to JSON format with comprehensive error handling.\n",
    "    Now accepts the nested org/repo structure.\n",
    "    \n",
    "    Args:\n",
    "        coms_data: Dictionary with structure {\"org_name\": {\"repo_name\": {\"commits\": [...]}}}\n",
    "        num_commits: Number of commits to sample per repository\n",
    "        workers: Number of parallel workers for processing\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing the structured JSON data\n",
    "    \"\"\"\n",
    "    result = {\n",
    "        'errors': [],\n",
    "        'repos': []\n",
    "    }\n",
    "    \n",
    "    # Validate input\n",
    "    if not coms_data:\n",
    "        result['errors'].append(\"No repository data provided\")\n",
    "        return result\n",
    "    \n",
    "    if num_commits <= 0:\n",
    "        result['errors'].append(f\"Invalid num_commits value: {num_commits}\")\n",
    "        return result\n",
    "    \n",
    "    # Process each org/repo in the nested structure\n",
    "    for org_name, repos in coms_data.items():\n",
    "        for repo_name, repo_data in repos.items():\n",
    "            try:\n",
    "                if not repo_data or 'commits' not in repo_data:\n",
    "                    result['errors'].append(f\"Skipping invalid repository data for {org_name}/{repo_name}\")\n",
    "                    continue\n",
    "                    \n",
    "                commits = repo_data['commits']\n",
    "                \n",
    "                if not commits:\n",
    "                    result['errors'].append(f\"No commits found for repository: {org_name}/{repo_name}\")\n",
    "                    continue\n",
    "                \n",
    "                actual_num_commits = min(num_commits, len(commits))\n",
    "                if actual_num_commits < num_commits:\n",
    "                    result['errors'].append(\n",
    "                        f\"Requested {num_commits} commits but only {len(commits)} available for {org_name}/{repo_name}\"\n",
    "                    )\n",
    "                \n",
    "                repo_entry = {\n",
    "                    'repo_name': repo_name,\n",
    "                    'org_name': org_name,\n",
    "                    'commits': []\n",
    "                }\n",
    "                \n",
    "                for commit_info in commits:\n",
    "                    try:\n",
    "                        try:\n",
    "                            file_names = Commit.get_file_names_from_commit(\n",
    "                                f\"download\\\\orgs\\\\{org_name}\\\\{repo_name}\",\n",
    "                                commit_info[0]\n",
    "                            )\n",
    "                        except Exception as e:\n",
    "                            result['errors'].append(\n",
    "                                f\"Error getting files for commit {commit_info[0]} in {org_name}/{repo_name}: {str(e)}\"\n",
    "                            )\n",
    "                            file_names = []\n",
    "                        \n",
    "                        if not file_names:\n",
    "                            result['errors'].append(\n",
    "                                f\"No files found for commit {commit_info[0]} in {org_name}/{repo_name}\"\n",
    "                            )\n",
    "                            continue\n",
    "                        \n",
    "                        try:\n",
    "                            # Note: You'll need to adapt get_com_cfs_and_hunks to work with org/repo format\n",
    "                            cfs, hunks = get_com_cfs_and_hunks(\n",
    "                                f\"download\\\\orgs\\\\{org_name}\\\\{repo_name}\",\n",
    "                                Commit(commit_info[0], repo_name, org_name, datetime.now(pytz.utc), commit_info[1]),\n",
    "                                file_names,\n",
    "                                workers\n",
    "                            )\n",
    "                        except Exception as e:\n",
    "                            result['errors'].append(\n",
    "                                f\"Error processing content for commit {commit_info[0]} in {org_name}/{repo_name}: {str(e)}\"\n",
    "                            )\n",
    "                            continue\n",
    "                        \n",
    "                        commit_entry = {\n",
    "                            'message': '',  # You might need to fetch this if not in coms_data\n",
    "                            'sha': commit_info[0],\n",
    "                            'link': f\"https://github.com/{org_name}/{repo_name}/commit/{commit_info[0]}\",\n",
    "                            'files': []\n",
    "                        }\n",
    "                        \n",
    "                        file_content_map = {}\n",
    "                        \n",
    "                        for cf in cfs or []:\n",
    "                            try:\n",
    "                                if not hasattr(cf, 'file_name') or not hasattr(cf, 'content'):\n",
    "                                    continue\n",
    "                                    \n",
    "                                file_content_map[cf.file_name] = {\n",
    "                                    'name': cf.file_name,\n",
    "                                    'content': {\n",
    "                                        'current': cf.content,\n",
    "                                        'diffs': []\n",
    "                                    }\n",
    "                                }\n",
    "                            except Exception as e:\n",
    "                                result['errors'].append(\n",
    "                                    f\"Error processing file content for {cf.file_name} in commit {commit_info[0]}: {str(e)}\"\n",
    "                                )\n",
    "                        \n",
    "                        for hunk in hunks or []:\n",
    "                            try:\n",
    "                                if (hasattr(hunk, 'file_name') and \n",
    "                                    hasattr(hunk, 'lines') and \n",
    "                                    hunk.file_name in file_content_map):\n",
    "                                    file_content_map[hunk.file_name]['content']['diffs'].append(hunk.lines)\n",
    "                            except Exception as e:\n",
    "                                result['errors'].append(\n",
    "                                    f\"Error processing hunk for file {hunk.file_name} in commit {commit_info[0]}: {str(e)}\"\n",
    "                                )\n",
    "                        \n",
    "                        commit_entry['files'] = list(file_content_map.values())\n",
    "                        \n",
    "                        if commit_entry['files']:\n",
    "                            repo_entry['commits'].append(commit_entry)\n",
    "                        else:\n",
    "                            result['errors'].append(\n",
    "                                f\"No valid files found for commit {commit_info[0]} in {org_name}/{repo_name}\"\n",
    "                            )\n",
    "                    \n",
    "                    except Exception as e:\n",
    "                        result['errors'].append(\n",
    "                            f\"Unexpected error processing commit {commit_info[0]} in {org_name}/{repo_name}: {str(e)}\"\n",
    "                        )\n",
    "                        continue\n",
    "                \n",
    "                if repo_entry['commits']:\n",
    "                    result['repos'].append(repo_entry)\n",
    "                else:\n",
    "                    result['errors'].append(\n",
    "                        f\"No valid commits found for repository {org_name}/{repo_name}\"\n",
    "                    )\n",
    "            \n",
    "            except Exception as e:\n",
    "                result['errors'].append(\n",
    "                    f\"Unexpected error processing repository {org_name}/{repo_name}: {str(e)}\"\n",
    "                )\n",
    "                continue\n",
    "    \n",
    "    if result['errors']:\n",
    "        logging.warning(f\"Encountered {len(result['errors'])} errors during export:\")\n",
    "        for error in result['errors']:\n",
    "            logging.warning(f\" - {error}\")\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09edda32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_json_in_file(data: Dict[str, Any], file_path: str):\n",
    "    \"\"\"\n",
    "    Save the structured JSON data to a file.\n",
    "    \n",
    "    Args:\n",
    "        data: Dictionary containing the structured JSON data\n",
    "        file_path: Path to the output JSON file\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'w') as f:\n",
    "            import json\n",
    "            json.dump(data, f, indent=4)\n",
    "        print(f\"Data successfully saved to {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving data to {file_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e58ec33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_workers = get_optimal_max_workers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61fcc2c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'spring-guides': {'tut-spring-boot-kotlin': {'commits': [('6bf4f5e34d7fed318eb0edcc8afb9f177c5448b9', 'Remove unneeded header'), ('e0990f9d1f7a2881ba154512662536d04a738fa4', 'Update start.spring.io and change package'), ('9a4d7411c9b0cb3b4885cb862bcec1ba5249069b', 'Fix broken Spring Initializr command line support link'), ('3d4b305b04bb7ff064140b0b5c088585ae6d79fe', 'Update README.adoc (#32)\\r\\nObvious Fix. \"properties.title\" is not accessible on this step. It should be updated in the end of the guide when \"private val properties: BlogProperties\" is defined for HtmlController class.'), ('b8b31b2000f616a87f60ceacea688537d773aad2', 'Update README.adoc\\r\\ntypo fix'), ('388c3958406b748f4a06d614a46d924133588819', 'Upgrade dependencies'), ('a25d4ef81458d7b0a1a05cc87bb9195b89404efb', 'Fix images location\\r\\n'), ('4fc16001420797eef32ec69f33b898007f632806', 'Clarify initializer settings and update image'), ('afcffbae5ae185ebd921f3de3a04a42329d04e5a', 'URL Cleanup\\r\\nThis commit updates URLs to prefer the https protocol. Redirects are not followed to avoid accidentally expanding intentionally shortened URLs (i.e. if using a URL shortener).\\r\\n\\r\\n# Fixed URLs\\r\\n\\r\\n## Fixed Success\\r\\nThese URLs were switched to an https URL with a 2xx status. While the status was successful, your review is still recommended.\\r\\n\\r\\n* http://www.apache.org/licenses/LICENSE-2.0 migrated to:\\r\\n  https://www.apache.org/licenses/LICENSE-2.0 ([https](https://www.apache.org/licenses/LICENSE-2.0) result 200).\\r\\n\\r\\n# Ignored\\r\\nThese URLs were intentionally ignored.\\r\\n\\r\\n* http://maven.apache.org/POM/4.0.0\\r\\n* http://maven.apache.org/xsd/maven-4.0.0.xsd\\r\\n* http://www.w3.org/2001/XMLSchema-instance'), ('a71b0c1fb27bb68049efeee7514781372198fc56', 'Removed extra spaces around application.properties code snippet to maintain consistent formatting')]}}, 'googlesamples': {'android-custom-lint-rules': {'commits': [('499d2b30a38f85c6679786dc6804cf001074b564', \"Polishing custom lint rules sample naming, comments, and use of LintUtils.\\r\\n- Using Collections.singleton(List) where appropriate\\r\\n- Clarify meaning of 'main activity'\\r\\n- Use LintUtils\\r\\n- Formatting\\r\\n\\r\\nChange-Id: Iab1a82f619e103cf6401f68c36704402f6c87252\"), ('af20a35ea03ad10798626c330d9e4e22a1c637a9', 'Added dependency descriptions to the README and a note in each build.gradle to reference the README.'), ('bfcb1f2f01460e2cb3a3aa5f1f36fa3d52eff069', 'Removes unused import.\\r\\nChange-Id: I6e90be2217be00e5c823f6d70bc0791cc583e266'), ('12f29057b5d68c9169a5728597764fc009ad4589', 'Fix typos in docs'), ('9f855da4801e58ed6f78476e37b8318c5e15ea91', 'Upgrade Maven build to Spring Boot 2.1.2 as well'), ('e01536a9676fa5e2ac13d5315c71688a9b38314b', \"Add sample for Android Studio 3.x\\r\\nAs of 3.0 beta 5, we finally have support for automatically packaging\\r\\nand running lint checks in your project, such that you don't have to\\r\\nadd your own local jar copy tasks anymore (or worse, dropping it in\\r\\n~/.android/lint as the current version of this sample describes).\\r\\n\\r\\nThis CL moves the current sample into an android-studio-2 folder,\\r\\nand adds a new adnroid-studio-3 folder with a project showing the\\r\\nnew project layout.\"), ('ed5fc41bf6f040e56c4b4c729c9e575f89c2b3ff', 'Merge pull request #85 from googlesamples/kts\\r\\nUpdate build files to KTS and version catalogs (and latest AGP)'), ('95a20e4c3fd3b61ad23c8dd8c0a2e5ccafdc0282', 'Update lint to latest versions of AGP.\\r\\nAlso cleans up a few other things and updates the\\r\\ndocumentation snapshot.'), ('07b0ddc1f429c9729ef3b401e826bec5757c84e2', 'Merge pull request #37 from tnorbye/main\\r\\nAdd github pages to serve documentation'), ('5e5e6e8fa25c4f003d9b66bc9de92e7a0517bc88', 'Fix typo in changes.md')]}}, 'aws-samples': {'aws-marketplace-serverless-saas-integration': {'commits': [('b76fb7c3ef021558dbeb9a3b790aa114deb378df', 'removed stack name dependency and replaced with unique ID segment from stack arn instead'), ('67488b67f0e7c8413276e02e4ef48d322b70e0d4', 'parameterized the sns topics account id and region'), ('b2de6bd851f7c55db8567d12c86c8b9c19d470dc', 'Update readme to cover approving SNS and adding CNAME record'), ('32e465a27b518a8bb04df4262ba4500fa34b987a', 'removed sam package command and modified sam deploy to remove --template-file'), ('c31ca403783ea5e360bb677d28fc2e35ee974851', \"Parameter was added to control fulfilment page url update. Scoped down Customer Resource's Policy\"), ('45bce421453542e9eef6e57af8dd6d03cbf55883', 'fixed numbering'), ('276cd9a481e26d023eb03720c1e9978f0c8cc56c', 'Spelling updates'), ('e1905bf112a184a0941f1cc212e5d4105462a589', 'Fixing DDBWritePolicy SID Type for Issue #32 (#35)'), ('9f0fc059a11641164984496c517555fb8674840e', 'Lambda functions updated to node18'), ('e66fd3be328fc0cac95737aacc37eeff0ac00658', 'Adding wording that SES email must be in production mode, not sandbox mode')]}}, 'Azure-Samples': {'azure-search-python-samples': {'commits': [('c9473ba9a9072283f48782f82d6f34d0c66187de', 'edits'), ('60c948ac8334e35cee97e1ace8065b4226815202', 'Commit made from VS Code Azure Static Web Apps'), ('8728dd278f41b55b2ad17b6b5878d0bba5ce8690', 'added the merge skill back in'), ('041fdf2765f3eb6339252928145c0a8e0c738ab8', 'Added Python Cognitive API tutorial example + updated readme file'), ('e58b5c70079b61a3c20b46b93f359848cf2d1ccc', 'removed cell outputs from notebook'), ('a2d4365eba95c1ab72c70841088439d0cdce53d6', 'Updated readme to link to new v11 code'), ('d0eb94c7eabb236d70afb5263cc1fb482f89b02f', 'run black'), ('b763ef6551720481473d594eb4477974a3169b4a', ' Merge pull request #92 from HeidiSteen/main\\r\\nFixed syntax errors'), ('1f116d8582c5b30853e9532a0f0ea2ad3cf59975', 'ci: add Azure Static Web Apps workflow file\\r\\non-behalf-of: @Azure opensource@microsoft.com'), ('770ebce1e967c55f361d939030684eb4f95b8d33', 'Merge pull request #132 from HeidiSteen/main\\r\\nMore comments, explanations and updates to RAG tutorial')]}}}\n"
     ]
    }
   ],
   "source": [
    "df_commits = pd.read_csv('data/chosen_commits.csv')\n",
    "coms_data = {}\n",
    "\n",
    "for _, row in df_commits.iterrows():\n",
    "    org = row['org_name']\n",
    "    repo = row['repo_name']\n",
    "    sha = row['sha']\n",
    "    message = row['message']\n",
    "    \n",
    "    if org not in coms_data:\n",
    "        coms_data[org] = {}\n",
    "    \n",
    "    if repo not in coms_data[org]:\n",
    "        coms_data[org][repo] = { \"commits\": [] }\n",
    "    \n",
    "    coms_data[org][repo][\"commits\"].append((sha, message))\n",
    "\n",
    "print(coms_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76e04432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully saved to golden_set.json\n"
     ]
    }
   ],
   "source": [
    "json_data = export_to_json(coms_data, 10, max_workers)\n",
    "output_file = 'golden_set.json'\n",
    "save_json_in_file(json_data, output_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
