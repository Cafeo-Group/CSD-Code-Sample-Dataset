{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1f21885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added parent directory 'finer' to sys.path\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import random\n",
    "from typing import List, Dict, Any\n",
    "import logging\n",
    "\n",
    "# Get the current notebook's directory\n",
    "notebook_dir = os.getcwd()\n",
    "parent_dir = str(Path(notebook_dir).parent)\n",
    "\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "    print(f\"Added parent directory '{Path(parent_dir).name}' to sys.path\")\n",
    "    \n",
    "os.chdir(parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "428f4fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.repository import Repository\n",
    "from models.commit import Commit\n",
    "from models.file import File\n",
    "from models.cf import CommitFile, MetadataHelper\n",
    "from models.hunk import Hunk\n",
    "from utils.worker import get_optimal_max_workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aae82131",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_commmit_link(repo, sha):\n",
    "    return f\"https://github.com/{repo.org_name}/{repo.repo_name}/commit/{sha}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fcbd28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_repository(row):\n",
    "    repo = Repository.csv_row_to_Repository(row)\n",
    "    repo_path = repo.get_repo_path()\n",
    "\n",
    "    try:\n",
    "        print(f\"Processing repository: {repo_path}\")\n",
    "        commits = Commit.get_commit_data(repo_path, datetime.now(pytz.timezone(\"UTC\")), True)\n",
    "\n",
    "        return {\n",
    "            'repo': repo,\n",
    "            'commits': commits\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {repo_path}: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "154e4833",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_metadata(mt, com, name, file_content):\n",
    "    cf = CommitFile(com.repo_name, com.org_name, name, com.sha, file_content, mt.change_type, mt.file_mode, mt.index_info)\n",
    "    hunk = Hunk(None, name, com.repo_name, com.org_name, com.sha, mt.old_start, mt.old_length, mt.new_start, mt.new_length, mt.lines, mt.old_name, mt.new_name)\n",
    "    return cf, hunk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b72fdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_com_cfs_and_hunks(repo, com, file_names, workers):\n",
    "    cfs = []\n",
    "    hunks = []\n",
    "\n",
    "    for name in file_names:\n",
    "        file_content, _ = File.get_file_content(repo.get_repo_path(), com.sha, name)\n",
    "        metadata_list = CommitFile.get_metadata(com.org_name, com.repo_name, com.sha, name, True)\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=workers) as executor:\n",
    "            futures = [executor.submit(process_metadata, mt, com, name, file_content) for mt in metadata_list]\n",
    "            for future in futures:\n",
    "                try:\n",
    "                    cf, hunk = future.result()\n",
    "                    cfs.append(cf)\n",
    "                    hunks.append(hunk)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing metadata for file {name}: {e}\")\n",
    "\n",
    "    return cfs, hunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ddc45793",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_to_json(repos_data: List[Dict[str, Any]], num_commits: int, workers: int) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Export repository data to JSON format with comprehensive error handling.\n",
    "    \n",
    "    Args:\n",
    "        repos_data: List of repository data dictionaries\n",
    "        num_commits: Number of commits to sample per repository\n",
    "        workers: Number of parallel workers for processing\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing the structured JSON data\n",
    "    \"\"\"\n",
    "    # Initialize result structure\n",
    "    result = {\n",
    "        'errors': [],  # Track any errors that occurred during processing\n",
    "        'repos': []\n",
    "    }\n",
    "    \n",
    "    # Validate input\n",
    "    if not repos_data:\n",
    "        result['errors'].append(\"No repository data provided\")\n",
    "        return result\n",
    "    \n",
    "    if num_commits <= 0:\n",
    "        result['errors'].append(f\"Invalid num_commits value: {num_commits}\")\n",
    "        return result\n",
    "    \n",
    "    for data in repos_data:\n",
    "        try:\n",
    "            if not data or 'repo' not in data or 'commits' not in data:\n",
    "                result['errors'].append(f\"Skipping invalid repository data: {data}\")\n",
    "                continue\n",
    "                \n",
    "            repo = data['repo']\n",
    "            commits = data['commits']\n",
    "            \n",
    "            # Skip if no commits available\n",
    "            if not commits:\n",
    "                result['errors'].append(f\"No commits found for repository: {repo.repo_name}\")\n",
    "                continue\n",
    "            \n",
    "            # Handle case where requested commits exceed available commits\n",
    "            actual_num_commits = min(num_commits, len(commits))\n",
    "            if actual_num_commits < num_commits:\n",
    "                result['errors'].append(\n",
    "                    f\"Requested {num_commits} commits but only {len(commits)} available for {repo.repo_name}\"\n",
    "                )\n",
    "            \n",
    "            try:\n",
    "                selected_commits = random.sample(commits, actual_num_commits)\n",
    "            except ValueError as e:\n",
    "                result['errors'].append(\n",
    "                    f\"Error sampling commits for {repo.repo_name}: {str(e)}\"\n",
    "                )\n",
    "                continue\n",
    "            \n",
    "            repo_entry = {\n",
    "                'repo_name': repo.repo_name if hasattr(repo, 'repo_name') else '',\n",
    "                'org_name': repo.org_name if hasattr(repo, 'org_name') else '',\n",
    "                'commits': []\n",
    "            }\n",
    "            \n",
    "            for commit in selected_commits:\n",
    "                try:\n",
    "                    # Get file names for this commit\n",
    "                    try:\n",
    "                        file_names = Commit.get_file_names_from_commit(\n",
    "                            repo.get_repo_path(), \n",
    "                            commit.sha\n",
    "                        ) if hasattr(repo, 'get_repo_path') else []\n",
    "                    except Exception as e:\n",
    "                        result['errors'].append(\n",
    "                            f\"Error getting files for commit {commit.sha} in {repo.repo_name}: {str(e)}\"\n",
    "                        )\n",
    "                        file_names = []\n",
    "                    \n",
    "                    if not file_names:\n",
    "                        result['errors'].append(\n",
    "                            f\"No files found for commit {commit.sha} in {repo.repo_name}\"\n",
    "                        )\n",
    "                        continue\n",
    "                    \n",
    "                    # Get content and hunks\n",
    "                    try:\n",
    "                        cfs, hunks = get_com_cfs_and_hunks(\n",
    "                            repo, \n",
    "                            commit, \n",
    "                            file_names, \n",
    "                            workers\n",
    "                        )\n",
    "                    except Exception as e:\n",
    "                        result['errors'].append(\n",
    "                            f\"Error processing content for commit {commit.sha} in {repo.repo_name}: {str(e)}\"\n",
    "                        )\n",
    "                        continue\n",
    "                    \n",
    "                    # Organize commit data\n",
    "                    commit_entry = {\n",
    "                        'message': getattr(commit, 'message', ''),\n",
    "                        'sha': getattr(commit, 'sha', ''),\n",
    "                        'link': f\"https://github.com/{repo.org_name}/{repo.repo_name}/commit/{commit.sha}\",\n",
    "                        'files': []\n",
    "                    }\n",
    "                    \n",
    "                    # Group content by file\n",
    "                    file_content_map = {}\n",
    "                    \n",
    "                    # Process current file contents\n",
    "                    for cf in cfs or []:\n",
    "                        try:\n",
    "                            if not hasattr(cf, 'file_name') or not hasattr(cf, 'content'):\n",
    "                                continue\n",
    "                                \n",
    "                            file_content_map[cf.file_name] = {\n",
    "                                'name': cf.file_name,\n",
    "                                'content': {\n",
    "                                    'current': cf.content,\n",
    "                                    'diffs': []\n",
    "                                }\n",
    "                            }\n",
    "                        except Exception as e:\n",
    "                            result['errors'].append(\n",
    "                                f\"Error processing file content for {cf.file_name} in commit {commit.sha}: {str(e)}\"\n",
    "                            )\n",
    "                    \n",
    "                    # Process hunks/diffs\n",
    "                    for hunk in hunks or []:\n",
    "                        try:\n",
    "                            if (hasattr(hunk, 'file_name') and \n",
    "                                hasattr(hunk, 'lines') and \n",
    "                                hunk.file_name in file_content_map):\n",
    "                                file_content_map[hunk.file_name]['content']['diffs'].append(hunk.lines)\n",
    "                        except Exception as e:\n",
    "                            result['errors'].append(\n",
    "                                f\"Error processing hunk for file {hunk.file_name} in commit {commit.sha}: {str(e)}\"\n",
    "                            )\n",
    "                    \n",
    "                    # Add files to commit entry\n",
    "                    commit_entry['files'] = list(file_content_map.values())\n",
    "                    \n",
    "                    # Only add commit if we have files\n",
    "                    if commit_entry['files']:\n",
    "                        repo_entry['commits'].append(commit_entry)\n",
    "                    else:\n",
    "                        result['errors'].append(\n",
    "                            f\"No valid files found for commit {commit.sha} in {repo.repo_name}\"\n",
    "                        )\n",
    "                \n",
    "                except Exception as e:\n",
    "                    result['errors'].append(\n",
    "                        f\"Unexpected error processing commit {getattr(commit, 'sha', 'unknown')} in {repo.repo_name}: {str(e)}\"\n",
    "                    )\n",
    "                    continue\n",
    "            \n",
    "            # Only add repo if we have commits\n",
    "            if repo_entry['commits']:\n",
    "                result['repos'].append(repo_entry)\n",
    "            else:\n",
    "                result['errors'].append(\n",
    "                    f\"No valid commits found for repository {repo.repo_name}\"\n",
    "                )\n",
    "        \n",
    "        except Exception as e:\n",
    "            result['errors'].append(\n",
    "                f\"Unexpected error processing repository {getattr(repo, 'repo_name', 'unknown')}: {str(e)}\"\n",
    "            )\n",
    "            continue\n",
    "    \n",
    "    # Log all errors at the end\n",
    "    if result['errors']:\n",
    "        logging.warning(f\"Encountered {len(result['errors'])} errors during export:\")\n",
    "        for error in result['errors']:\n",
    "            logging.warning(f\" - {error}\")\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09edda32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_json_in_file(data: Dict[str, Any], file_path: str):\n",
    "    \"\"\"\n",
    "    Save the structured JSON data to a file.\n",
    "    \n",
    "    Args:\n",
    "        data: Dictionary containing the structured JSON data\n",
    "        file_path: Path to the output JSON file\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'w') as f:\n",
    "            import json\n",
    "            json.dump(data, f, indent=4)\n",
    "        print(f\"Data successfully saved to {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving data to {file_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e58ec33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "repos = pd.read_csv('./code_samples.csv', skiprows=1)\n",
    "repos = repos.dropna(subset=['html_url'])\n",
    "max_workers = get_optimal_max_workers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98e5a234",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_repos = [\n",
    "    'aws-marketplace-serverless-saas-integration',\n",
    "    'azure-search-python-samples',\n",
    "    'android-custom-lint-rules',\n",
    "    'eign-eureka',\n",
    "    'tut-spring-boot-kotlin'\n",
    "]\n",
    "selected_rows = [row for _, row in repos.iterrows() if row['name'] in selected_repos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "919a5099",
   "metadata": {},
   "outputs": [],
   "source": [
    "repos_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5e257a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing repository: download\\orgs\\googlesamples\\android-custom-lint-rules\n",
      "Processing repository: download\\orgs\\aws-samples\\aws-marketplace-serverless-saas-integration\n",
      "Processing repository: download\\orgs\\Azure-Samples\\azure-search-python-samples\n",
      "Processing repository: download\\orgs\\spring-guides\\tut-spring-boot-kotlin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing repositories:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing repositories: 100%|██████████| 4/4 [00:00<00:00,  4.20it/s]\n"
     ]
    }
   ],
   "source": [
    "with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "    futures = {executor.submit(process_repository, row): row['name'] for row in selected_rows}\n",
    "    for future in tqdm(as_completed(futures), total=len(futures), desc=\"Processing repositories\"):\n",
    "        result = future.result()\n",
    "        if result:\n",
    "            repos_data.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5775eea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collected data from 4 repositories.\n",
      "Repo commits:\n",
      "      \n",
      "tut-spring-boot-kotlin (108), \n",
      "android-custom-lint-rules (132), \n",
      "aws-marketplace-serverless-saas-integration (162), \n",
      "azure-search-python-samples (324)\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nCollected data from {len(repos_data)} repositories.\")\n",
    "print(f\"\"\"Repo commits:\n",
    "      \n",
    "{', \\n'.join([f\"{data['repo'].repo_name} ({len(data['commits'])})\" for data in repos_data])}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "33e8874d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_names = Commit.get_file_names_from_commit(\n",
    "#     f\"download\\\\orgs\\\\googlesamples\\\\android-custom-lint-rules\", \n",
    "#     \"ed5fc41bf6f040e56c4b4c729c9e575f89c2b3ff\"\n",
    "# )\n",
    "# print(f\"File names in commit: {file_names}\")\n",
    "\n",
    "# commit = Commit(\"ed5fc41bf6f040e56c4b4c729c9e575f89c2b3ff\", \"android-custom-lint-rules\", \"googlesamples\", datetime.now(pytz.timezone(\"UTC\")), f\"\"\"Merge pull request #85 from googlesamples/kts\n",
    "# Update build files to KTS and version catalogs (and latest AGP)\"\"\")\n",
    "\n",
    "# def test(repo_path, com, file_names, workers):\n",
    "#     cfs = []\n",
    "#     hunks = []\n",
    "\n",
    "#     for name in file_names:\n",
    "#         file_content, _ = File.get_file_content(repo_path, com.sha, name)\n",
    "#         metadata_list = CommitFile.get_metadata(com.org_name, com.repo_name, com.sha, name, True)\n",
    "#         print(f\"Processing file: {name} with metadata: {metadata_list}\")\n",
    "\n",
    "#         with ThreadPoolExecutor(max_workers=workers) as executor:\n",
    "#             futures = [executor.submit(process_metadata, mt, com, name, file_content) for mt in metadata_list]\n",
    "#             for future in futures:\n",
    "#                 try:\n",
    "#                     cf, hunk = future.result()\n",
    "#                     cfs.append(cf)\n",
    "#                     hunks.append(hunk)\n",
    "#                 except Exception as e:\n",
    "#                     print(f\"Error processing metadata for file {name}: {e}\")\n",
    "\n",
    "#     return cfs, hunks\n",
    "\n",
    "# cfs, hunks = test(f\"download\\\\orgs\\\\googlesamples\\\\android-custom-lint-rules\", commit, file_names, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76e04432",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Encountered 2 errors during export:\n",
      "WARNING:root: - No valid files found for commit 7c9c47b0d66c25e7673ca91f9e8b08cd0e084817 in android-custom-lint-rules\n",
      "WARNING:root: - No files found for commit 75c82dea3a0df5452273cec5c05415ccfa7d419d in azure-search-python-samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully saved to exported_data.json\n"
     ]
    }
   ],
   "source": [
    "json_data = export_to_json(repos_data, 10, max_workers)\n",
    "output_file = 'exported_data.json'\n",
    "save_json_in_file(json_data, output_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
