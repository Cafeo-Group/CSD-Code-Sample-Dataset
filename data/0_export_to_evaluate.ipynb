{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1f21885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added parent directory 'finer' to sys.path\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import random\n",
    "from typing import List, Dict, Any\n",
    "import logging\n",
    "\n",
    "notebook_dir = os.getcwd()\n",
    "parent_dir = str(Path(notebook_dir).parent)\n",
    "\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "    print(f\"Added parent directory '{Path(parent_dir).name}' to sys.path\")\n",
    "    \n",
    "os.chdir(parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "428f4fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.repository import Repository\n",
    "from models.commit import Commit\n",
    "from models.file import File\n",
    "from models.cf import CommitFile, MetadataHelper\n",
    "from models.hunk import Hunk\n",
    "from utils.worker import get_optimal_max_workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aae82131",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_commmit_link(repo, sha):\n",
    "    return f\"https://github.com/{repo.org_name}/{repo.repo_name}/commit/{sha}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "154e4833",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_metadata(mt, com, name, file_content):\n",
    "    cf = CommitFile(com.repo_name, com.org_name, name, com.sha, file_content, mt.change_type, mt.file_mode, mt.index_info)\n",
    "    hunk = Hunk(None, name, com.repo_name, com.org_name, com.sha, mt.old_start, mt.old_length, mt.new_start, mt.new_length, mt.lines, mt.old_name, mt.new_name)\n",
    "    return cf, hunk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b72fdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_com_cfs_and_hunks(repo_path, com, file_names, workers):\n",
    "    cfs = []\n",
    "    hunks = []\n",
    "\n",
    "    for name in file_names:\n",
    "        file_content, _ = File.get_file_content(repo_path, com.sha, name)\n",
    "        metadata_list = CommitFile.get_metadata(com.org_name, com.repo_name, com.sha, name, True)\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=workers) as executor:\n",
    "            futures = [executor.submit(process_metadata, mt, com, name, file_content) for mt in metadata_list]\n",
    "            for future in futures:\n",
    "                try:\n",
    "                    cf, hunk = future.result()\n",
    "                    cfs.append(cf)\n",
    "                    hunks.append(hunk)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing metadata for file {name}: {e}\")\n",
    "\n",
    "    return cfs, hunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddc45793",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_to_json(coms_data: Dict[str, Dict[str, Dict[str, List[str]]]], num_commits: int, workers: int) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Export repository data to JSON format with comprehensive error handling.\n",
    "    Now accepts the nested org/repo structure.\n",
    "    \n",
    "    Args:\n",
    "        coms_data: Dictionary with structure {\"org_name\": {\"repo_name\": {\"commits\": [...]}}}\n",
    "        num_commits: Number of commits to sample per repository\n",
    "        workers: Number of parallel workers for processing\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing the structured JSON data\n",
    "    \"\"\"\n",
    "    result = {\n",
    "        'errors': [],\n",
    "        'repos': []\n",
    "    }\n",
    "    \n",
    "    if not coms_data:\n",
    "        result['errors'].append(\"No repository data provided\")\n",
    "        return result\n",
    "    \n",
    "    if num_commits <= 0:\n",
    "        result['errors'].append(f\"Invalid num_commits value: {num_commits}\")\n",
    "        return result\n",
    "    \n",
    "    for org_name, repos in coms_data.items():\n",
    "        for repo_name, repo_data in repos.items():\n",
    "            try:\n",
    "                if not repo_data or 'commits' not in repo_data:\n",
    "                    result['errors'].append(f\"Skipping invalid repository data for {org_name}/{repo_name}\")\n",
    "                    continue\n",
    "                    \n",
    "                commits = repo_data['commits']\n",
    "                \n",
    "                if not commits:\n",
    "                    result['errors'].append(f\"No commits found for repository: {org_name}/{repo_name}\")\n",
    "                    continue\n",
    "                \n",
    "                actual_num_commits = min(num_commits, len(commits))\n",
    "                if actual_num_commits < num_commits:\n",
    "                    result['errors'].append(\n",
    "                        f\"Requested {num_commits} commits but only {len(commits)} available for {org_name}/{repo_name}\"\n",
    "                    )\n",
    "                \n",
    "                repo_entry = {\n",
    "                    'repo_name': repo_name,\n",
    "                    'org_name': org_name,\n",
    "                    'commits': []\n",
    "                }\n",
    "                \n",
    "                for commit_info in commits:\n",
    "                    try:\n",
    "                        try:\n",
    "                            file_names = Commit.get_file_names_from_commit(\n",
    "                                f\"download\\\\orgs\\\\{org_name}\\\\{repo_name}\",\n",
    "                                commit_info[0]\n",
    "                            )\n",
    "                        except Exception as e:\n",
    "                            result['errors'].append(\n",
    "                                f\"Error getting files for commit {commit_info[0]} in {org_name}/{repo_name}: {str(e)}\"\n",
    "                            )\n",
    "                            file_names = []\n",
    "                        \n",
    "                        if not file_names:\n",
    "                            result['errors'].append(\n",
    "                                f\"No files found for commit {commit_info[0]} in {org_name}/{repo_name}\"\n",
    "                            )\n",
    "                            continue\n",
    "                        \n",
    "                        try:\n",
    "                            cfs, hunks = get_com_cfs_and_hunks(\n",
    "                                f\"download\\\\orgs\\\\{org_name}\\\\{repo_name}\",\n",
    "                                Commit(commit_info[0], repo_name, org_name, datetime.now(pytz.utc), commit_info[1]),\n",
    "                                file_names,\n",
    "                                workers\n",
    "                            )\n",
    "                        except Exception as e:\n",
    "                            result['errors'].append(\n",
    "                                f\"Error processing content for commit {commit_info[0]} in {org_name}/{repo_name}: {str(e)}\"\n",
    "                            )\n",
    "                            continue\n",
    "                        \n",
    "                        commit_entry = {\n",
    "                            'message': commit_info[1],\n",
    "                            'sha': commit_info[0],\n",
    "                            'link': f\"https://github.com/{org_name}/{repo_name}/commit/{commit_info[0]}\",\n",
    "                            'files': [],\n",
    "                            'what': commit_info[2],\n",
    "                            'why': commit_info[3],\n",
    "                            'files_changed': commit_info[4]\n",
    "                        }\n",
    "                        \n",
    "                        file_content_map = {}\n",
    "                        \n",
    "                        for cf in cfs or []:\n",
    "                            try:\n",
    "                                if not hasattr(cf, 'file_name') or not hasattr(cf, 'content'):\n",
    "                                    continue\n",
    "                                    \n",
    "                                file_content_map[cf.file_name] = {\n",
    "                                    'name': cf.file_name,\n",
    "                                    'content': {\n",
    "                                        'current': cf.content,\n",
    "                                        'diffs': []\n",
    "                                    }\n",
    "                                }\n",
    "                            except Exception as e:\n",
    "                                result['errors'].append(\n",
    "                                    f\"Error processing file content for {cf.file_name} in commit {commit_info[0]}: {str(e)}\"\n",
    "                                )\n",
    "                        \n",
    "                        for hunk in hunks or []:\n",
    "                            try:\n",
    "                                if (hasattr(hunk, 'file_name') and \n",
    "                                    hasattr(hunk, 'lines') and \n",
    "                                    hunk.file_name in file_content_map):\n",
    "                                    file_content_map[hunk.file_name]['content']['diffs'].append(hunk.lines)\n",
    "                            except Exception as e:\n",
    "                                result['errors'].append(\n",
    "                                    f\"Error processing hunk for file {hunk.file_name} in commit {commit_info[0]}: {str(e)}\"\n",
    "                                )\n",
    "                        \n",
    "                        commit_entry['files'] = list(file_content_map.values())\n",
    "                        \n",
    "                        if commit_entry['files']:\n",
    "                            repo_entry['commits'].append(commit_entry)\n",
    "                        else:\n",
    "                            result['errors'].append(\n",
    "                                f\"No valid files found for commit {commit_info[0]} in {org_name}/{repo_name}\"\n",
    "                            )\n",
    "                    except Exception as e:\n",
    "                        result['errors'].append(\n",
    "                            f\"Unexpected error processing commit {commit_info[0]} in {org_name}/{repo_name}: {str(e)}\"\n",
    "                        )\n",
    "                        continue\n",
    "                \n",
    "                if repo_entry['commits']:\n",
    "                    result['repos'].append(repo_entry)\n",
    "                else:\n",
    "                    result['errors'].append(\n",
    "                        f\"No valid commits found for repository {org_name}/{repo_name}\"\n",
    "                    )\n",
    "            \n",
    "            except Exception as e:\n",
    "                result['errors'].append(\n",
    "                    f\"Unexpected error processing repository {org_name}/{repo_name}: {str(e)}\"\n",
    "                )\n",
    "                continue\n",
    "    \n",
    "    if result['errors']:\n",
    "        logging.warning(f\"Encountered {len(result['errors'])} errors during export:\")\n",
    "        for error in result['errors']:\n",
    "            logging.warning(f\" - {error}\")\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09edda32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_json_in_file(data: Dict[str, Any], file_path: str):\n",
    "    \"\"\"\n",
    "    Save the structured JSON data to a file.\n",
    "    \n",
    "    Args:\n",
    "        data: Dictionary containing the structured JSON data\n",
    "        file_path: Path to the output JSON file\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'w') as f:\n",
    "            import json\n",
    "            json.dump(data, f, indent=4)\n",
    "        print(f\"Data successfully saved to {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving data to {file_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e58ec33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_workers = get_optimal_max_workers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61fcc2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_commits = pd.read_csv('data/chosen_commits.csv')\n",
    "coms_data = {}\n",
    "\n",
    "for _, row in df_commits.iterrows():\n",
    "    org = row['org_name']\n",
    "    repo = row['repo_name']\n",
    "    sha = row['sha']\n",
    "    message = row['message']\n",
    "    what = row['what']\n",
    "    why = row['why']\n",
    "    files_changed = row['files_changed']\n",
    "    \n",
    "    if org not in coms_data:\n",
    "        coms_data[org] = {}\n",
    "    \n",
    "    if repo not in coms_data[org]:\n",
    "        coms_data[org][repo] = { \"commits\": [] }\n",
    "    \n",
    "    coms_data[org][repo][\"commits\"].append((sha, message, what, why, files_changed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76e04432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully saved to evaluate_set.json\n"
     ]
    }
   ],
   "source": [
    "json_data = export_to_json(coms_data, 10, max_workers)\n",
    "output_file = 'evaluate_set.json'\n",
    "save_json_in_file(json_data, output_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
