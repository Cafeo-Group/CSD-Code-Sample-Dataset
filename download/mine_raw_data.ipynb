{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install GitPython pandas tqdm pathlib psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from git import Repo\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "from dataclasses import dataclass\n",
    "import pickle\n",
    "from typing import List\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import subprocess\n",
    "import psutil\n",
    "import gc\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def db_conn():\n",
    "    return psycopg2.connect(\n",
    "        database = 'code_samples',\n",
    "        user = 'postgres',\n",
    "        host = 'localhost',\n",
    "        password = 'codesamples',\n",
    "        port = '5432'        \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clone(gitUrl: str, repoDir: str, sample: str) -> None:\n",
    "    '''Clone a git repository and checkout all files in the repository\n",
    "    \n",
    "    Args:\n",
    "    gitUrl (str): URL of the git repository\n",
    "    repoDir (str): Directory to clone the repository to\n",
    "    sample (str): Name of the sample\n",
    "        \n",
    "    Returns:\n",
    "        None'''\n",
    "    repo_path = os.path.join(repoDir, sample)\n",
    "    os.makedirs(repo_path, exist_ok=True)\n",
    "\n",
    "    repo = Repo.clone_from(gitUrl, repo_path, multi_options=[\"--no-checkout\"])\n",
    "\n",
    "    try:\n",
    "        repo.git.reset('--hard', 'HEAD') # Reset the working tree to HEAD\n",
    "\n",
    "        repo.git.checkout('--', '.') # Partial checkout in batches\n",
    "    except Exception as e:\n",
    "        print(f\"Error checking out files for {sample}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download(sample: str) -> None:\n",
    "    '''Download the repository\n",
    "    \n",
    "    Args:\n",
    "    sample (str): Name of the sample\n",
    "    \n",
    "    Returns:\n",
    "        None'''\n",
    "    gitHubUrl = f\"https://github.com/{sample}.git\"\n",
    "    repoDir = \"repositories/\"\n",
    "    isdir = os.path.isdir(repoDir+sample)\n",
    "    if isdir:\n",
    "        return\n",
    "    else:\n",
    "        clone(gitHubUrl, repoDir, sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class RawData:\n",
    "    full_path: str\n",
    "    timestamp: datetime\n",
    "    sha: str\n",
    "    message: str\n",
    "    diff: str\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"-{self.sha}\\n- {self.message}\\n- {self.timestamp}\\n- {self.diff}\"\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__str__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data_in_batches(batch_data: List[RawData], batch_index: int):\n",
    "    with open(f'raw_data_batch_{batch_index}.pkl', 'wb') as p:\n",
    "        pickle.dump(batch_data, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_memory():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    memory_used_mb = process.memory_info().rss / 1024 / 1024\n",
    "    return memory_used_mb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def free_memory():\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = db_conn()\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(f\"\"\"CREATE TABLE IF NOT EXISTS raw_data (\n",
    "     full_path TEXT,\n",
    "     timestamp TIMESTAMP,\n",
    "     sha TEXT,\n",
    "     message TEXT,\n",
    "     diff TEXT\n",
    "     );\"\"\")\n",
    "conn.commit()\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_commit(commit: RawData):\n",
    "    conn = db_conn()\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(f\"\"\"INSERT INTO raw_data (full_path, timestamp, sha, message, diff) VALUES (%s, %s, %s, %s, %s)\"\"\",\n",
    "                   (commit.full_path, commit.timestamp, commit.sha, commit.message, commit.diff))\n",
    "    conn.commit()\n",
    "    cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw_data(repo_path: str, cutoff_date: datetime, batch_size: int = 100) -> None:\n",
    "    if not os.path.exists(os.path.join(repo_path, '.git')):\n",
    "        print(f\"Skipping non-Git directory: {repo_path}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # Checks if the repo has any commits\n",
    "        subprocess.check_output([\"git\", \"rev-parse\", \"HEAD\"], cwd=repo_path)\n",
    "\n",
    "        process = subprocess.Popen(\n",
    "            [\"git\", \"log\", \"--pretty=format:%H<<DELIM>>%ct<<DELIM>>%s\", \"--patch\", f\"--until={cutoff_date.timestamp()}\"],\n",
    "            cwd=repo_path,\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE\n",
    "        )\n",
    "\n",
    "        commit_info = []\n",
    "        for line in process.stdout:\n",
    "            commit_info.append(line.decode('utf-8', errors='replace'))\n",
    "\n",
    "        process.wait()\n",
    "\n",
    "        commit_info = ''.join(commit_info).split('\\n\\n')\n",
    "\n",
    "        batch_data = []\n",
    "        batch_index = 0\n",
    "\n",
    "        for entry in commit_info:\n",
    "            if entry:\n",
    "                parts = entry.split('<<DELIM>>')\n",
    "                if len(parts) < 3:\n",
    "                    print(f\"Skipping malformed entry: {entry}\")\n",
    "                    continue\n",
    "\n",
    "                sha, timestamp, message = parts[:3]\n",
    "                commit_datetime = datetime.fromtimestamp(int(timestamp), tz=pytz.utc)\n",
    "\n",
    "                diff = '\\n'.join(parts[3:]).strip()\n",
    "\n",
    "                raw_data = RawData(\n",
    "                    full_path=repo_path,\n",
    "                    timestamp=commit_datetime.isoformat(),\n",
    "                    sha=sha,\n",
    "                    message=message,\n",
    "                    diff=diff\n",
    "                )\n",
    "                \n",
    "\n",
    "                batch_data.append(raw_data)\n",
    "                if len(batch_data) >= batch_size:\n",
    "                    save_data_in_batches(batch_data, batch_index)\n",
    "                    batch_data = []  # Reset batch\n",
    "\n",
    "        # Save any remaining data in the last batch\n",
    "        if batch_data:\n",
    "            save_data_in_batches(batch_data, batch_index)\n",
    "            batch_index += 1\n",
    "        else:\n",
    "            print(f\"Error processing {repo_path}: {entry}\")\n",
    "\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error processing {repo_path}: {e}\")\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_repos_raw_data(parent_folder: str) -> None:\n",
    "    \"\"\"\n",
    "    Processes all repositories in a parent folder and gathers RawData for each commit.\n",
    "\n",
    "    Args:\n",
    "    parent_folder (str): The path to the folder containing all repositories.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    repo_paths = []\n",
    "    \n",
    "    for sub_dir in os.listdir(parent_folder):\n",
    "        sub_dir_path = os.path.join(parent_folder, sub_dir)\n",
    "        if os.path.isdir(sub_dir_path):\n",
    "            for repo_dir in os.listdir(sub_dir_path):\n",
    "                repo_dir_path = os.path.join(sub_dir_path, repo_dir)\n",
    "                if os.path.isdir(repo_dir_path) and os.path.exists(os.path.join(repo_dir_path, '.git')):\n",
    "                    repo_paths.append(repo_dir_path)\n",
    "\n",
    "    counter = 0\n",
    "    with ThreadPoolExecutor(max_workers=7) as executor:\n",
    "        future_to_repo = {executor.submit(get_raw_data, repo, datetime(2024, 9, 19, tzinfo=pytz.UTC)): repo for repo in repo_paths}\n",
    "        for future in tqdm(as_completed(future_to_repo), total=len(future_to_repo), desc=\"Processing Repositories\"):\n",
    "            counter += future.result()\n",
    "            \n",
    "    free_memory()\n",
    "    print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repos = pd.read_csv('../code_samples.csv', skiprows=1)\n",
    "repos = repos.dropna(subset=['html_url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(len(repos)), desc=f\"Downloading Repositories\"):\n",
    "    repo = repos.iloc[i]\n",
    "    repo_ecosystem = repo['html_url'].split('/')[-2]\n",
    "    repo_name = repo['name']\n",
    "    sample_name = f\"{repo_ecosystem}/{repo_name}\"\n",
    "    download(sample_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_raw_data = get_all_repos_raw_data(os.path.join('repositories'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('raw_data_batch_1.pkl', 'rb') as p:\n",
    "    batch_data = pickle.load(p)\n",
    "    print(batch_data[18])\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
